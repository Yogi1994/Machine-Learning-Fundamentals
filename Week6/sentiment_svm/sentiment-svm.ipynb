{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with support vector machines\n",
    "\n",
    "In this notebook, we will revisit a learning task that we encountered earlier in the course: predicting the *sentiment* (positive or negative) of a single sentence taken from a review of a movie, restaurant, or product. The data set consists of 3000 labeled sentences, which we divide into a training set of size 2500 and a test set of size 500. Previously we found a logistic regression classifier. Today we will use a support vector machine.\n",
    "\n",
    "Before starting on this notebook, make sure the folder `sentiment_labelled_sentences` (containing the data file `full_set.txt`) is in the same directory. Recall that the data can be downloaded from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing the data\n",
    " \n",
    "Here we follow exactly the same steps as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4500)\n",
      "test data:  (500, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Read in the data set.\n",
    "with open(\"sentiment labelled sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "## Read in the data set.\n",
    "with open(\"sentiment labelled sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]\n",
    "\n",
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "\n",
    "## Append '1' to the end of each vector.\n",
    "data_mat = data_features.toarray()\n",
    "\n",
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a support vector machine to the data\n",
    "\n",
    "In support vector machines, we are given a set of examples $(x_1, y_1), \\ldots, (x_n, y_n)$ and we want to find a weight vector $w \\in \\mathbb{R}^d$ that solves the following optimization problem:\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^d} \\| w \\|^2 + C \\sum_{i=1}^n \\xi_i $$\n",
    "$$ \\text{subject to } y_i \\langle w, x_i \\rangle \\geq 1 - \\xi_i \\text{ for all } i=1,\\ldots, n$$\n",
    "\n",
    "`scikit-learn` provides an SVM solver that we will use. The following routine takes as input the constant `C` (from the above optimization problem) and returns the training and test error of the resulting SVM model. It is invoked as follows:\n",
    "\n",
    "* `training_error, test_error = fit_classifier(C)`\n",
    "\n",
    "The default value for parameter `C` is 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "def fit_classifier(C_value=1.0):\n",
    "    clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "    clf.fit(train_data,train_labels)\n",
    "    ## Get predictions on training data\n",
    "    train_preds = clf.predict(train_data)\n",
    "    train_error = float(np.sum((train_preds > 0.0) != (train_labels > 0.0)))/len(train_labels)\n",
    "    ## Get predictions on test data\n",
    "    test_preds = clf.predict(test_data)\n",
    "    test_error = float(np.sum((test_preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "    ##\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for C = 0.01: train 0.214 test 0.254\n",
      "Error rate for C = 0.10: train 0.068 test 0.190\n",
      "Error rate for C = 1.00: train 0.010 test 0.156\n",
      "Error rate for C = 10.00: train 0.001 test 0.180\n",
      "Error rate for C = 100.00: train 0.001 test 0.182\n",
      "Error rate for C = 1000.00: train 0.002 test 0.176\n",
      "Error rate for C = 10000.00: train 0.001 test 0.170\n"
     ]
    }
   ],
   "source": [
    "cvals = [0.01,0.1,1.0,10.0,100.0,1000.0,10000.0]\n",
    "for c in cvals:\n",
    "    train_error, test_error = fit_classifier(c)\n",
    "    print (\"Error rate for C = %0.2f: train %0.3f test %0.3f\" % (c, train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating C by k-fold cross-validation\n",
    "\n",
    "As we can see, the choice of `C` has a very significant effect on the performance of the SVM classifier. We were able to assess this because we have a separate test set. In general, however, this is a luxury we won't possess. How can we choose `C` based only on the training set?\n",
    "\n",
    "A reasonable way to estimate the error associated with a specific value of `C` is by **`k-fold cross validation`**:\n",
    "* Partition the training set `S` into `k` equal-sized sized subsets `S_1, S_2, ..., S_k`.\n",
    "* For `i=1,2,...,k`, train a classifier with parameter `C` on `S - S_i` (all the training data except `S_i`) and test it on `S_i` to get error estimate `e_i`.\n",
    "* Average the errors: `(e_1 + ... + e_k)/k`\n",
    "\n",
    "The following procedure, **cross_validation_error**, does exactly this. It takes as input:\n",
    "* the training set `x,y`\n",
    "* the value of `C` to be evaluated\n",
    "* the integer `k`\n",
    "\n",
    "and it returns the estimated error of the classifier for that particular setting of `C`. <font color=\"magenta\">Look over the code carefully to understand exactly what it is doing.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_error(x,y,C_value,k):\n",
    "    n = len(y)\n",
    "    ## Randomly shuffle indices\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    ## Initialize error\n",
    "    err = 0.0\n",
    "    \n",
    "    ## Iterate over partitions\n",
    "    for i in range(k):\n",
    "        ## Partition indices\n",
    "        test_indices = indices[int(i*(n/k)):int((i+1)*(n/k) - 1)]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "        \n",
    "        ## Train classifier with parameter c\n",
    "        clf = svm.LinearSVC(C=C_value, loss='hinge')\n",
    "        clf.fit(x[train_indices], y[train_indices])\n",
    "        \n",
    "        ## Get predictions on test partition\n",
    "        preds = clf.predict(x[test_indices])\n",
    "        \n",
    "        ## Compute error\n",
    "        err += float(np.sum((preds > 0.0) != (y[test_indices] > 0.0)))/len(test_indices)\n",
    "        \n",
    "    return err/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Picking a value of C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure **cross_validation_error** (above) evaluates a single candidate value of `C`. We need to use it repeatedly to identify a good `C`. \n",
    "\n",
    "<font color=\"magenta\">**For you to do:**</font> Write a function to choose `C`. It will be invoked as follows:\n",
    "\n",
    "* `c, err = choose_parameter(x,y,k)`\n",
    "\n",
    "where\n",
    "* `x,y` is the training data\n",
    "* `k` is the number of folds of cross-validation\n",
    "* `c` is chosen value of the parameter `C`\n",
    "* `err` is the cross-validation error estimate at `c`\n",
    "\n",
    "<font color=\"magenta\">Note:</font> This is a tricky business because a priori, even the order of magnitude of `C` is unknown. Should it be 0.0001 or 10000? You might want to think about trying multiple values that are arranged in a geometric progression (such as powers of ten). *In addition to returning a specific value of `C`, your function should **plot** the cross-validation errors for all the values of `C` it tried out (possibly using a log-scale for the `C`-axis).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_parameter(x,y,k):\n",
    "    C = [0.01, 0.03, 0.09, 0.1, 0.3, 0.9, 1.0, 1.1, 1.3, 3.0, 9.0, 10.0,]\n",
    "    err = []\n",
    "    mini = 0\n",
    "    for c in C:\n",
    "        err.append(cross_validation_error(x,y,c,k))\n",
    "        if err[mini] > err[len(err)-1]:\n",
    "            mini = len(err)-1\n",
    "    \n",
    "    plt.plot(C, err)\n",
    "    return C[mini], err[mini]\n",
    "    \n",
    "    \n",
    "    ### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out your routine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice of C:  0.3\n",
      "Cross-validation error estimate:  0.18514056224899594\n",
      "Test error:  0.156\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8bfa9e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c, err = choose_parameter(train_data, train_labels, 10)\n",
    "print(\"Choice of C: \", c)\n",
    "print(\"Cross-validation error estimate: \", err)\n",
    "## Train it and test it\n",
    "clf = svm.LinearSVC(C=c, loss='hinge')\n",
    "clf.fit(train_data, train_labels)\n",
    "preds = clf.predict(test_data)\n",
    "error = float(np.sum((preds > 0.0) != (test_labels > 0.0)))/len(test_labels)\n",
    "print(\"Test error: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to ponder:**</font> How does the plot of cross-validation errors for different `C` look? Is there clearly a trough in which the returned value of `C` falls? Does the plot provide some reassurance that the choice is reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "1px",
    "right": "20px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
